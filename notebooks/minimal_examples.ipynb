{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Cell Type classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import tensorflow as tf; tf.logging.set_verbosity(tf.logging.ERROR)  # suppress deprecation messages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from depiction.models.base.base_model import BaseModel\n",
    "from depiction.models.examples.celltype.celltype import CellTyper\n",
    "from depiction.core import Task, DataType\n",
    "from depiction.interpreters.backprop.backpropeter import BackPropeter\n",
    "from depiction.models.keras.core import KerasModel\n",
    "from depiction.explanations.feature_attribution import aggregate_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "datapath = '../data/single-cell/data.csv'\n",
    "data_df = pd.read_csv(datapath)\n",
    "\n",
    "#scale the data from 0 to 1\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "data = min_max_scaler.fit_transform(data_df.drop('category', axis=1).values)\n",
    "data_df = pd.DataFrame(\n",
    "    np.append(data, data_df['category'].values[:, None], axis=1), index=data_df.index, columns=data_df.columns\n",
    ")\n",
    "\n",
    "#  split as in traing of the model\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.33, random_state=42, stratify=data_df.category)\n",
    "test_df, valid_df = train_test_split(test_df, test_size=0.67, random_state=42, stratify=test_df.category)\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "markers = train_df.columns[:-1]\n",
    "\n",
    "X_train = train_df[markers].values\n",
    "X_test = test_df[markers].values\n",
    "X_valid = valid_df[markers].values\n",
    "\n",
    "y_train = train_df['category'].values.astype(np.int)\n",
    "y_test = test_df['category'].values.astype(np.int)\n",
    "y_valid = valid_df['category'].values.astype(np.int)\n",
    "\n",
    "sample_id = 4\n",
    "sample = X_test[sample_id:sample_id+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import trained classifier\n",
    "classifier = CellTyper(filename='celltype_model.h5')\n",
    "print(classifier.celltype_names)\n",
    "classifier = KerasModel(classifier.model, Task.CLASSIFICATION, DataType.TABULAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create backpropagation-based explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['saliency','shapley_sampling', 'occlusion', 'elrp']\n",
    "\n",
    "explanations = []\n",
    "for m in methods:\n",
    "    interpreter = BackPropeter(classifier, m)\n",
    "    exp = interpreter.interpret(sample)[0].make_positive().normalize()\n",
    "    explanations.append(exp)\n",
    "    \n",
    "explanations = aggregate_attributions(explanations, mode='none')\n",
    "fig, ax = explanations.visualize(feature_names=markers, y_labels=methods, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting protein binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import tensorflow as tf; tf.logging.set_verbosity(tf.logging.ERROR)  # suppress deprecation messages\n",
    "from depiction.models.examples.deepbind.deepbind import DeepBind, create_DNA_language\n",
    "from depiction.interpreters.u_wash.u_washer import UWasher, to_feature_attribution\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_foxa1 = DeepBind('DeepBind/Homo_sapiens/TF/D00761.001_ChIP-seq_FOXA1', min_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['NOT BINDING', 'BINDING']\n",
    "lime_explanation_configs = {\n",
    "    'labels': (1,),\n",
    "}\n",
    "lime_params = {\n",
    "    'class_names': class_names,\n",
    "    'split_expression': list,\n",
    "    'bow': False,\n",
    "    'char_level': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = UWasher(\"lime\", classifier_foxa1, **lime_params)\n",
    "classifier_foxa1.use_labels = False\n",
    "explanation = lime_explainer.interpret(\"TGTTTACTTT\", explanation_configs=lime_explanation_configs)\n",
    "explanation = to_feature_attribution(explanation, classifier_foxa1.data_type, labels=[1])\n",
    "fig, ax = explanation.visualize(tokens=list(\"TGTTTACTTT\"), show=True, as_logo=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
